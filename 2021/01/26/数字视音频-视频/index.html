<!DOCTYPE html>
<html lang="zh-CN">





<head>
    <meta charset="UTF-8">
    <meta name="google-site-verification" content="27z_W5AnQzBSH6OUQE1R9SFm194-UyXDI_iVWTSZ4qw" />
    <link rel="apple-touch-icon" sizes="76x76" href="/silmont2000.github.io/img/apple-touch-icon.png">
    <link rel="icon" type="image/png" href="/silmont2000.github.io/img/favicon.jpg">
    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <meta name="theme-color" content="#2f4154">
    <meta name="description" content=" 找个地方记录自己的学习和成长&lt;br&gt; 写不出东西的时候就去草坪晒个太阳&lt;br&gt; 堕落街吃个鸡丝拌面缙云烧饼摩天脆脆&lt;br&gt; 之后虽然还是写不出来&lt;br&gt; 但日子又好过很多了&lt;br&gt;">
    <meta name="author" content="Silmont">
    <meta name="keywords" content="">
    <title>数字视音频-视频 - 今天也有晒太阳欸</title>

    <link  rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/5.12.1/css/all.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/mdbootstrap/4.13.0/css/mdb.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/3.0.1/github-markdown.min.css" />

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link  rel="stylesheet" href="/silmont2000.github.io/lib/prettify/tomorrow-night-eighties.min.css" />

<link  rel="stylesheet" href="/silmont2000.github.io/css/main.css" />


  <link defer rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />


<!-- 自定义样式保持在最底部 -->


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/silmont2000.github.io/">&nbsp;<strong>小谢的零食屋qwq</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/silmont2000.github.io/">首页</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/silmont2000.github.io/archives/">归档</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/silmont2000.github.io/categories/">分类</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/silmont2000.github.io/tags/">标签</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/silmont2000.github.io/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/silmont2000.github.io/img/default.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
                <p class="mt-3 post-meta">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>
                  星期二, 一月 26日 2021, 3:11 下午
                </p>
              

              <p class="mt-1">
                
                  
                  <span class="post-meta">
                    <i class="far fa-chart-bar"></i>
                    6.7k 字
                  </span>
                

                
                  
                  <span class="post-meta">
                      <i class="far fa-clock"></i>
                      24 分钟
                  </span>
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  <span id="busuanzi_container_page_pv" class="post-meta" style="display: none">
                    <i class="far fa-eye" aria-hidden="true"></i>
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5 z-depth-3" id="board">
          <div class="post-content mx-auto" id="post">
            
            <div class="markdown-body">
              <p>[TOC]</p>
<h1 id="为啥"><a href="#为啥" class="headerlink" title="为啥"></a>为啥</h1><h2 id="为什么要搞压缩"><a href="#为什么要搞压缩" class="headerlink" title="为什么要搞压缩"></a>为什么要搞压缩</h2><ol>
<li>不压缩就太大了</li>
<li>不压缩可能会引起一些网络通信问题</li>
<li>HDTV计算：每秒30帧*每帧1920*1080像素*每个像素每个通道8比特*3个通道=1492992000bps，太大了</li>
<li>信道的最大信息速率才19.2Mb，用18来算就是要达到83:1的压缩比才行</li>
</ol>
<h2 id="为什么视频这么大"><a href="#为什么视频这么大" class="headerlink" title="为什么视频这么大"></a>为什么视频这么大</h2><ol>
<li><p>数据中的统计冗余或结构（空间、时间、光谱）</p>
</li>
<li><p>感知无关信息的存在</p>
<p><strong>所以，我们要把冗余空间信息·冗余时间信息·颜色信息扔掉。</strong></p>
<ol>
<li><p>冗余空间信息：SIF：288*352</p>
</li>
<li><p>颜色：对亮度最敏感，颜色不那么重要、RGB编码冗余，411采样YUV</p>
</li>
<li><p>计算：</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127083240759.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127083240759"></p>
</li>
<li><p>可行性：视频数据的时间冗余、静态图像本身的冗余</p>
</li>
</ol>
</li>
</ol>
<h1 id="视频压缩"><a href="#视频压缩" class="headerlink" title="视频压缩"></a>视频压缩</h1><p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127083535086.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127083535086"></p>
<p>输入-压缩编码-存储或通过网络传递-解码解压-输出</p>
<h2 id="无损压缩"><a href="#无损压缩" class="headerlink" title="无损压缩"></a>无损压缩</h2><h3 id="游程编码-无损"><a href="#游程编码-无损" class="headerlink" title="游程编码-无损"></a>游程编码-无损</h3><p>数字写前面。</p>
<p>Example:</p>
<p>– Original data:</p>
<p>WWWWWWWWWWWWBWWWWWWWWWWWWBBBWWWWWWWWWWWWW</p>
<p>WWWWWWWWWWWBWWWWWWWWWWWWWW</p>
<p>– Its run-length code:</p>
<p>12W1B12W3B24W1B14W</p>
<h3 id="预测编码-无损"><a href="#预测编码-无损" class="headerlink" title="预测编码-无损"></a>预测编码-无损</h3><p>重建=预测+误差，存误差。预测公式可以是每个都减左边那个，然后用自适应算数编码压缩误差数据。</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127084151206.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127084151206"></p>
<p>对视频最简单的编码：按时间顺序减去图像-编码残差，称为：</p>
<h3 id="差分编码-无损"><a href="#差分编码-无损" class="headerlink" title="差分编码-无损"></a>差分编码-无损</h3><p>对变化快的无能为力</p>
<h3 id="哈弗曼编码-无损"><a href="#哈弗曼编码-无损" class="headerlink" title="哈弗曼编码-无损"></a>哈弗曼编码-无损</h3><p><a href="https://blog.csdn.net/FX677588/article/details/70767446" target="_blank" rel="noopener">https://blog.csdn.net/FX677588/article/details/70767446</a></p>
<p>更经常发生的符号将具有较短的代码·</p>
<p>过程：-根据发生概率对符号进行排序-将最小概率符号组合成一个复合符号，概率等于相应符号概率之和-对剩余符号重复过程-从代码的二叉树表示中提取代码</p>
<p>时刻调整</p>
<h2 id="有损压缩"><a href="#有损压缩" class="headerlink" title="有损压缩"></a>有损压缩</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>根据信息理论的原理-编码向量比编码标量更有效-需要将连续的样本从输入到向量中分组。</p>
<p>让X=样本的x1，x2，…，xkbe向量，相邻之间存在数量相关性。 如果Y是输入向量的线性变换T的结果，其分量的相关性要小得多，那么Y可以比X更有效地编码。 </p>
<p><strong>原因：相关性越大，冗余越多。</strong></p>
<p>转换T本身不压缩任何数据。 压缩来自Y组件的处理和量化。 DCT是一种广泛使用的变换，它可以对输入信号进行去相关。</p>
<p>傅里叶变换：</p>
<p>余弦基是正交的，相乘=0，自己相乘=1</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127100609534.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127100609534"></p>
<p>保留前6个参数就可以很好的还原了。</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127101823300.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127101823300"></p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127101948898.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127101948898"></p>
<h3 id="二维DCT"><a href="#二维DCT" class="headerlink" title="二维DCT"></a>二维DCT</h3><p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127102227213.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127102227213"></p>
<p>过程：先把图片8*8分块然后用公式变换：</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127102945705.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127102945705"></p>
<p>简化：分解成2个一维DCT</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127103246247.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127103246247"></p>
<p>之后对DCT结果进行量化，大量归零，再做IDCT</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127103347803.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127103347803"></p>
<p>这里的游程编码是看每个数前面有几个0，最后的一大堆0都可以不编码了</p>
<p><strong>应用：数字水印</strong></p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127103545238.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127103545238"></p>
<h1 id="基于运动补偿的视频压缩"><a href="#基于运动补偿的视频压缩" class="headerlink" title="基于运动补偿的视频压缩"></a>基于运动补偿的视频压缩</h1><p>视频是：时间维度上的图像序列</p>
<p>相邻帧之间的差异主要来自：相机或物体的运动</p>
<p>运动图片编码原则：减少空间冗余和时间冗余</p>
<p>​    -帧内：类似于JPEG</p>
<p>​    -帧间：基于运动预测和补偿</p>
<p>​        ·P帧、B帧·多帧引用H.264</p>
<h2 id="运动补偿的基本思路"><a href="#运动补偿的基本思路" class="headerlink" title="运动补偿的基本思路"></a>运动补偿的基本思路</h2><p>把移动图像划分为静态背景和移动前景。首先用基础的JPEG编码第一帧，作为参考帧，之后的每一帧图像都和参考帧做对比，相同的块用特殊编码标记，对不同的块才使用普通的编码。</p>
<h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ol>
<li>检测出图像的运动（运动矢量搜索）</li>
<li>基于运动补偿预测下一帧</li>
<li>推导出误差</li>
</ol>
<p><strong>运动补偿是在宏块级别执行的。</strong> </p>
<p>N*N图像块：默认情况下，亮度图像的N=16。 对于色度图像，如果采用4：2：0色子采样，则N=8</p>
<p>目前的图像帧称为目标帧。 在目标框架中的宏块与以前和（或)未来框架中最相似的宏块(称为参考框架)之间寻找匹配)）。 在前向预测中，参考框架被认为是以前的框架。 <strong>参考宏块向目标宏块的位移称为运动矢量MV</strong>。</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127173823343.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127173823343"></p>
<p>搜索一般会限定在一个范围内，如上图的-p，p之间。</p>
<h3 id="运动向量检索"><a href="#运动向量检索" class="headerlink" title="运动向量检索"></a>运动向量检索</h3><p><strong>匹配原则</strong></p>
<p>设宏块的大小为n*n，宏块的中心为P。以目标帧中正在查找的像素位置为中心，在参考帧的[-p,p]范围内移动宏块，找距离最小的宏块。</p>
<p><strong>顺序搜索</strong></p>
<p>顺序搜索参考框架中的整个(2p+1)x(2p+1)窗口（也称为完全搜索）。</p>
<p>将以窗口内每个位置为中心的宏块与目标帧中逐像素的宏块进行比较，然后计算它们和目标帧的MAD。 提供最小MAD的向量(i，j)被指定为目标帧中宏块的MV(u，v)。 顺序搜索方法是非常昂贵的-假设每个像素比较需要三个操作（减法、绝对值、加法），获得单个宏块的运动向量的成本是(2p+1)*(2p+1)*N2*3=&gt;O(p2N2)</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127174651308.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127174651308"></p>
<h3 id="对数搜索"><a href="#对数搜索" class="headerlink" title="对数搜索"></a>对数搜索</h3><p>一个代价更小的版本，这是次优的，但通常仍然有效。</p>
<p>运动矢量的二维对数搜索过程需要几次迭代，类似于二进制搜索：-如下图所示，搜索窗口中最初只有9个位置被用作基于MAD的搜索的种子；它们被标记为“1’。 在产生最小MAD的位置之后，新搜索区域的中心被移动到它，步长（“偏移”）被缩小到一半。 在下一次迭代中，9个新位置被标记为“2”等等。</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127175013976.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127175013976"></p>
<h3 id="分层搜索"><a href="#分层搜索" class="headerlink" title="分层搜索"></a>分层搜索</h3><p>可以受益于分层（多分辨率）方法，在这种方法中，可以从分辨率显著降低的图像中获得运动矢量的初始估计。 三级分层搜索，其中原始图像处于0级，第1级和第2级的图像是通过从以前的级别向下采样2倍获得的，初始搜索是在第2级进行的。 由于宏块的大小较小，p也可以按比例减小，因此所需的操作次数大大减少。需要指出，每一层都是对数搜索。</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127175611940.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127175611940"></p>
<h1 id="H-261"><a href="#H-261" class="headerlink" title="H.261"></a>H.261</h1><p>H.261：早期的数字视频压缩标准（成立于1990年），它的基于MC（运动补偿）的压缩原则保留在后来的所有视频压缩标准中。-该标准是为ISDN上的录像机、视频会议和其他视听服务设计的。-视频编解码器支持比特率p*64kbps，其中P范围从1到30(因此也称为P*64)。-要求视频编码器的延迟小于150ms，以便视频可以用于实时双向视频会议。</p>
<p>定义了两种类型的图像帧：帧内(I帧)和帧间(P帧)：</p>
<p>-I帧被视为独立的图像。 在每个I帧中应用类似于JPEG的变换编码方法，因此“内部”。 </p>
<p>-P帧不是独立的：用前向预测编码方法编码(允许从以前的P帧进行预测，而不仅仅是从以前的I帧进行预测)。 P帧编码包括时间冗余去除，而I帧编码只执行空间冗余去除。 </p>
<p>-H.261中的运动矢量总是以全像素为单位测量，它们的搜索范围为±15px。 为了避免编码错误的传播，I帧通常在视频的每一秒发送几次。</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127180106016.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127180106016"></p>
<h2 id="对I帧编码"><a href="#对I帧编码" class="headerlink" title="对I帧编码"></a>对I帧编码</h2><p>I帧的图像块是16*16，那么在YUV下采样后就会得到8*8的U和V块。对这些块进行DCT，然后参数量化、zigzag最后哈夫曼编码（熵编码）。</p>
<h2 id="对P帧编码"><a href="#对P帧编码" class="headerlink" title="对P帧编码"></a>对P帧编码</h2><p>P帧中的每一个宏块，都能找到一个运动向量。那么也就能得到一个误差块，对误差快哈弗曼编码。</p>
<p>P帧编码不同的宏块（而不是目标宏块本身）。 有时无法找到良好的匹配，即预测误差超过一定的可接受水平。 -然后对MB本身进行编码(将其视为内部MB)，在这种情况下，它被称为非运动补偿MB。 对于运动向量，差分MVD被发送到熵编码：·MVD=MV预测−MV真正。</p>
<h2 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h2><p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127181113811.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127181113811"></p>
<p>DC：直流系数,是第一个系数，剩下的都是交流系数</p>
<h2 id="H-261比特流语法"><a href="#H-261比特流语法" class="headerlink" title="H.261比特流语法"></a>H.261比特流语法</h2><p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127181836563.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127181836563"></p>
<h3 id="PSC、TR"><a href="#PSC、TR" class="headerlink" title="PSC、TR"></a>PSC、TR</h3><p>picture start code:图片开始界限</p>
<p>timporal reference: 时间戳</p>
<h3 id="GOB"><a href="#GOB" class="headerlink" title="GOB"></a>GOB</h3><p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127182424097.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127182424097"></p>
<p>group of block，一个GOB里面有11*3个宏块。计算题如下：</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127182821326.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127182821326"></p>
<p>每个GOB层有GBSC（start code）标记开始，GN标记数目（number），如果网络错误导致一些错误或一些位的丢失，H.261视频可以在下一个可识别的GOB中恢复和重新同步。-GQ表示GOB中使用的量化器，除非它被后续的MQ（宏块的量化器）覆盖。</p>
<h3 id="MB"><a href="#MB" class="headerlink" title="MB"></a>MB</h3><p>macroblock。address标记MB在GOB中的位置（不是图片中的位置！！），MQ是macroblock的量化器。每个MB包含6个16*16的block。</p>
<h3 id="B"><a href="#B" class="headerlink" title="B"></a>B</h3><p>对于每个16x16块，比特流以直流分量开始，然后是zerorun(Run)的长度对和AC的后续非零值(Level)，最后是块的结束(EOB)代码。 run范围为[0；63]。 level反映量化值-它的范围是[−127，127]，并且不是0.</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127190244117.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127190244117"></p>
<h1 id="MPEG"><a href="#MPEG" class="headerlink" title="MPEG"></a>MPEG</h1><p>– Moving Pictures Experts Group运动图像专家组</p>
<p>1998</p>
<p>从1988年的25名专家到大约200家公司的350多名专家</p>
<h2 id="MPEG1"><a href="#MPEG1" class="headerlink" title="MPEG1"></a>MPEG1</h2><p>CD/VCD 最高1.5Mbps，1.2M是视频流，256K是音频流。</p>
<p>五个部分：系统Systems（11172-1），视频Video（11172-2），Audio，Conformance testing （一致性测试），及软件模拟Software Simulation。</p>
<p>源输入格式-只支持<strong>非间隔视频</strong></p>
<p>-352*240为NTSC在30fps</p>
<p>-352*288为PAL在25fps</p>
<p>-4：2：0色度次采样</p>
<p>在H.261中，只是前向预测，而MPEG4前后都用上：双向运动补偿，基于B帧</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127191137953.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127191137953"></p>
<p>基于MC的B帧编码思想是：-来自B帧的每个MB将有最多两个运动向量(MV)（一个来自前向，一个来自后向预测）。 -如果匹配成功，则将发送两个MV，并对两个相应的匹配MBS进行平均，然后与目标MB进行比较，以产生预测误差。 -如果一个可接受的匹配只能在其中一个参考框架中找到，那么从正向预测或反向预测中只使用一个MV及其相应的MB.这两种MB都还是会产生差分，对16的差分DCT变换和编码。</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127192200275.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127192200275"></p>
<p><strong>注意：传递顺序</strong>：IPBBPBBIBB，也就是先编码一个I，后面三帧三帧都反过来。</p>
<h3 id="和H-261比较"><a href="#和H-261比较" class="headerlink" title="和H.261比较"></a>和H.261比较</h3><p>注意几个标准格式。</p>
<p>352是44个8。</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127192635916.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127192635916"></p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127193213505.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127193213505"></p>
<p>MPEG-1允许运动矢量具有亚像素精度（1/2像素）。 H.263的“双线性插值”技术可用于在半像素位置生成所需的值。 </p>
<p>与H.261中运动矢量的最大正负15像素范围相比，MPEG-1支持半像素精度的[−512，511.5]范围，以及全像素精度运动矢量的[−1,024，1,023。 </p>
<p>MPEG-1位流允许随机访问-由GOP层完成，其中每个GOP被时间编码。</p>
<p>P帧(20)的压缩率远远大于I帧(7)，是因为时间的冗余消除了。因为双向预测的优势，还有B的最低优先级，而B帧(50)比P帧还小。</p>
<p>mpeg1的每一张可以分成几片，分别独立编码。</p>
<h3 id="比特流"><a href="#比特流" class="headerlink" title="比特流"></a>比特流</h3><p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127193932552.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127193932552"></p>
<p>H261是图像-GOB-MB-B-(RUN,LEVEL)</p>
<p>picture就是frame，GOP存的IPB之类的IBBPBBPBBPBB，12帧。slice是GOB，是block的集合。所以MPEG后面还是GOB(slice)-MB-B的结构，前面加了GOP和P。</p>
<p>H.261给出的是图像分层，MPEG是视频分层。</p>
<p><a href="https://www.cnblogs.com/samyboy/p/5478458.html" target="_blank" rel="noopener">https://www.cnblogs.com/samyboy/p/5478458.html</a></p>
<p>清华课本：</p>
<p><a href="https://books.google.com.hk/books?id=adKjHT5zZekC&amp;pg=PA133&amp;lpg=PA133&amp;dq=%E5%9C%BA%E5%9B%BE%E5%83%8F%E7%9A%84%E5%9C%BA%E9%A2%84%E6%B5%8B&amp;source=bl&amp;ots=RBSauybRjc&amp;sig=ACfU3U2-dm4WOpJjeFCfCpb0o_vFvA0jNw&amp;hl=zh-CN&amp;sa=X&amp;ved=2ahUKEwji2L3th7zuAhWMBKYKHW0uDcIQ6AEwB3oECAYQAg#v=onepage&amp;q&amp;f=false" target="_blank" rel="noopener">https://books.google.com.hk/books?id=adKjHT5zZekC&amp;pg=PA133&amp;lpg=PA133&amp;dq=%E5%9C%BA%E5%9B%BE%E5%83%8F%E7%9A%84%E5%9C%BA%E9%A2%84%E6%B5%8B&amp;source=bl&amp;ots=RBSauybRjc&amp;sig=ACfU3U2-dm4WOpJjeFCfCpb0o_vFvA0jNw&amp;hl=zh-CN&amp;sa=X&amp;ved=2ahUKEwji2L3th7zuAhWMBKYKHW0uDcIQ6AEwB3oECAYQAg#v=onepage&amp;q&amp;f=false</a></p>
<h2 id="MPEG2"><a href="#MPEG2" class="headerlink" title="MPEG2"></a>MPEG2</h2><p>开始于1990年，93完成，为更高质量的视频比特率超过4Mbps。 满足数字电视/HDTV的压缩和比特率要求。不同的分辨率，不同的压缩复杂性。 广泛接受地面、卫星、有线网络其他应用交互式电视、DVD（数字视频光盘或数字通用光盘）。</p>
<p>MPEG3原本是为了更高比特率下的高清数字电视做准备的，被融合进了MPEG2中。所以没有MPEG3。</p>
<p>规定了七个配置属性，每个配置可以最多有4个级别。</p>
<h3 id="支持交错视频"><a href="#支持交错视频" class="headerlink" title="支持交错视频"></a>支持交错视频</h3><p>支持交错视频·MPEG-2必须支持交错视频，因为这是数字广播电视和HDTV的选择之一。 在交错视频中，每个帧由两个字段组成，称为顶部字段和底部字段。 在帧图中，来自两个字段的所有扫描线被交织成一个帧，然后被划分为16*16宏块，并使用MC进行编码。 如果每个字段被视为一个单独的图片，那么它被称为字段图片。</p>
<h3 id="五种预测模型"><a href="#五种预测模型" class="headerlink" title="五种预测模型"></a>五种预测模型</h3><p><strong>帧图像的帧预测</strong></p>
<p>与MPEG1运动补偿相同，适用于包含慢和中等物体的视频</p>
<p><strong>场图像的场预测</strong></p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127195553121.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127195553121"></p>
<p><strong>帧图像的场预测</strong></p>
<p>分别处理顶场和底场</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127200247490.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127200247490"></p>
<p><strong>16x8MC的场图</strong></p>
<p>对于运动是快速和不规则的</p>
<p><strong>双基预测</strong></p>
<p>MV用于导出计算的运动矢量CV</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127200612197.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127200612197"></p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127201307895.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127201307895"></p>
<h3 id="可伸缩性"><a href="#可伸缩性" class="headerlink" title="可伸缩性"></a>可伸缩性</h3><p>MPEG-2可伸缩编码：可以定义一个基础层和一个或多个增强层——也称为分层编码。</p>
<p> 基础层可以独立编码、传输和解码，以获得基本的视频质量。 增强层的编码和解码取决于底层或前一个增强层。 </p>
<p>基础层：直流分量？</p>
<p>可缩放编码对于在具有以下特性的网络上传输的MPEG-2视频特别有用：-具有非常不同比特率的网络。 具有可变比特率(VBR)信道的网络。 有嘈杂连接的网络。</p>
<ol>
<li><p><strong>信噪比可伸缩性增强层提供了更高的信噪比。</strong></p>
<p>信噪比可伸缩性：指基层上的增强/细化，以改善SNR。 MPEG-2信噪比可伸缩编码器将在两层产生输出比特流Bits_base和Bits_enhance：</p>
<p>在基层上，对DCT系数进行了粗量化，导致比特较少，视频质量相对较低。</p>
<p>然后对粗量化的DCT系数进行反量化(Q−1)，并将其馈送到增强层，与原DCT系数进行比较。</p>
<p>它们的差异被精细量化以产生DCT系数细化，在VLC之后，它成为称为Bits_enhance的比特流。</p>
</li>
<li><p><strong>空间可伸缩性-增强层提供了更高的空间分辨率。</strong></p>
</li>
<li><p><strong>时间可伸缩性-增强层促进了更高的帧速率。</strong></p>
<p>输入视频被暂时地解复用成两块，每个部分携带原始帧速率的一半。 基础层编码器对自己的输入视频执行正常的单层编码过程，并产生输出比特流Bits_base。 增强层匹配MB的预测可以通过两种方式获得：<strong>-层间MC（运动补偿）预测-组合MC预测和层间MC预测。</strong></p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127203301895.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127203301895"></p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127203354769.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127203354769"></p>
</li>
<li><p><strong>混合可伸缩性-上述三个可伸缩性中的任意两个的组合。</strong></p>
<p>上述三种可伸缩性中的任何两种都可以组合成混合可伸缩性：1。 时空混合可伸缩性。 2. 信噪比和空间混合可伸缩性。 3. 信噪比和时间混合可伸缩性。 通常采用三层混合编码器，由基层、增强层1和增强层2组成。</p>
</li>
<li><p>数据分区-量化的DCT系数被分割成分区。</p>
<p>基本分区包含低频DCT系数，增强分区包含高频DCT系数。 严格地说，数据分区不是分层编码，因为单一的视频数据流被简单地划分，在生成增强分区时不再依赖于基本分区。 用于噪声信道上的传输和渐进传输。</p>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127202958989.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127202958989"></p>
<h3 id="和MPEG1的区别"><a href="#和MPEG1的区别" class="headerlink" title="和MPEG1的区别"></a>和MPEG1的区别</h3><p>除了程序流之外，还向MPEG-2位流添加了传输流。 </p>
<p>支持4：2：2和4：4：4的色度次采样。 </p>
<p>更受限制的切片结构：MPEG-2切片必须在同一宏块行中开始和结束。 换句话说，图片的左边缘总是启动一个新的切片，MPEG-2中最长的切片只能有一行宏块。 </p>
<p>更灵活的视频格式：它支持DVD、ATV和HDTV定义的各种图像分辨率。</p>
<p>非线性量化-允许两种类型的尺度：1。 对于第一种类型，比例尺与MPEG-1中的比例尺相同，是[1,31]之间的整数，scale_i=i。 2. 对于第二种类型，存在非线性关系，即scale不是i。</p>
<h2 id="MPEG4"><a href="#MPEG4" class="headerlink" title="MPEG4"></a>MPEG4</h2><p>较新的标准。 除了压缩之外，还非常关注用户交互的问题。 MPEG-4在采用一种新的基于对象的编码方面与它的前辈不同：-提供更高的压缩比，也有利于数字视频的合成、操作、索引和检索。 任意形状编码-静态纹理编码-人脸对象编码和动画-身体对象编码和动画-MPEG-4视频的比特率现在涵盖了5kbps到10Mbps之间的很大范围。</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127204014325.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127204014325"></p>
<p>MPEG-4是一个全新的标准：(a)合成媒体对象以创建理想的视听场景。 (b)对这些媒体数据实体的码流进行复用和同步，以便能够以保证服务质量(QoS)进行传输。 (c)在接收端与视听场景互动—-提供了一个先进编码模块和音频和视频压缩算法工具箱。</p>
<h3 id="层级"><a href="#层级" class="headerlink" title="层级"></a>层级</h3><p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127204218867.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127204218867"></p>
<ol>
<li>视频对象序列(VS)-提供完整的MPEG-4视觉场景，其中可能包含二维或三维自然或合成对象。 </li>
<li>视频对象(VO)-场景中的特定对象，它可以是与场景的对象或背景相对应的任意（非矩形）形状。</li>
<li>视频对象层(VOL)-促进了一种支持（多层）可伸缩编码的方法。 一个VO可以在可伸缩编码下有多个VOL，也可以在不可伸缩编码下有一个VOL</li>
<li>视频对象平面组(GOV)-将视频对象平面组在一起（可选级别）。</li>
<li>视频对象平面(VOP)-特定时刻VO的快照。</li>
</ol>
<h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127204942935.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127204942935"></p>
<h1 id="镜头检测-关键帧提取"><a href="#镜头检测-关键帧提取" class="headerlink" title="镜头检测 关键帧提取"></a>镜头检测 关键帧提取</h1><p>目录结构：相似镜头聚类</p>
<p>镜头边缘分类：硬切：切是从一个场景到下一个场景的瞬时过渡。 淡出：淡出是场景与恒定图像（淡出)或恒定图像与场景(淡入)之间的逐渐过渡）。 溶解：溶解是从一个场景逐渐过渡到另一个场景，其中第一个场景消失，第二个场景消失。 另一个常见的场景中断是擦除，其中一行移动到屏幕上，新的场景出现在行后面。</p>
<p>聚类的优点：<img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127205504203.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127205504203"></p>
<h1 id="镜头结构化"><a href="#镜头结构化" class="headerlink" title="镜头结构化"></a>镜头结构化</h1><p><strong>结构化生成</strong></p>
<p>镜头切分，相似镜头成组，镜头组之间的联系</p>
<p>对MPEG视频文件进行解码，得到视频图像帧。通过镜头分割和关键帧提取，得到镜头关键帧。对关键帧进行聚类分析，生成视频类，接着构造时序结构图，添加入口出口。</p>
<p><strong>Table of Video Content：TOC</strong></p>
<p><a href="https://link.springer.com/article/10.1007/s005300050138" target="_blank" rel="noopener">https://link.springer.com/article/10.1007/s005300050138</a></p>
<p>（需要校网认证）</p>
<p>可以直接读第三页的<img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127211301271.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127211301271" style="zoom:67%;">，下面是关键部分的翻译。</p>
<p>能读原文还是读原文吧。</p>
<p>为了向用户提供对视频的更好访问，需要在语义级别上构建视频ToC。现有的场景级视频ToC构建方法可以分为基于模型和通用两大类。</p>
<h2 id="基于模型"><a href="#基于模型" class="headerlink" title="基于模型"></a>基于模型</h2><p>在基于模型的方法中，首先构建特定应用程序或域的先验模型。这样的模型指定了场景边界特征，基于该特征可以将非结构化视频流抽象为结构化表示。 Swangberg等人提出了这种方法的理论框架。在[14]中，它已经成功实现了许多有趣的应用，包括新闻视频解析[20]和电视足球节目解析[7]。由于此方法基于特定的应用程序模型，因此通常可以实现较高的准确性。但是，此方法的缺点之一是，对于每个应用程序，都需要先构建模型，然后才能进行解析过程。建模过程很耗时，并且需要良好的领域知识和经验。</p>
<h2 id="基于场景（通用）"><a href="#基于场景（通用）" class="headerlink" title="基于场景（通用）"></a>基于场景（通用）</h2><p>用于基于场景的视频ToC构造的另一种方法不需要这种显式的域模型。这种方法的三项开创性著作来自法国的IRIT [1]，普林斯顿大学和IBM [4，16-18]和东芝公司[2]。在[1]中，Aigrain等。提出一种基于多峰规则的方法。他们首先确定由媒体内容给出的本地（及时）规则；在[4，17]中，首先将视频流分割为镜头。然后，使用时间约束聚类将视觉上和时间上相邻的镜头构造为聚类。最后，基于聚类建立了场景过渡图（STG），并确定了切割边缘以构建场景结构。在[2]中，代替使用STG，辅助工具将交替模式的镜头分组为场景（它们称为脚本）。然后创建视频结构的2D表示，垂直显示场景，水平显示关键帧。基于场景的视频ToC相对于其他方法的优点总结如下：–其他方法产生的条目太多，无法有效地呈现镜头，关键帧甚至组仅传达物理上的不连续性，而场景传达语义上的不连续性，例如场景在时间和/或位置上的变化。</p>
<h2 id="本文"><a href="#本文" class="headerlink" title="本文"></a>本文</h2><p>它具有四个主要模块：镜头边界检测和关键帧提取，时空特征提取，时间自适应分组以及场景结构构建。我们在下面依次讨论每个模块。</p>
<h3 id="镜头检测和关键帧提取"><a href="#镜头检测和关键帧提取" class="headerlink" title="镜头检测和关键帧提取"></a>镜头检测和关键帧提取</h3><p>采用特征突变的地方作为边缘；为了速度，选择镜头的开始和结束作为2个关键帧。</p>
<h3 id="时空特征提取"><a href="#时空特征提取" class="headerlink" title="时空特征提取"></a>时空特征提取</h3><p>提取镜头的特征来表征时间信息：</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127211617748.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127211617748"></p>
<p>其中Acti和Ni是第i个镜头的活动性度量和帧数； Diffk,k-1是帧k和k-1之间的颜色直方图差异； Hist（k）和Hist（k-1）是帧kandk-1的颜色直方图； Dist（）是直方图之间的距离算法<strong>（这里用的是直方图求交）</strong>。在本文中，我们采用区间内距离。所使用的颜色直方图是沿HSV颜色空间中的H和S的二维直方图。我们忽略了V分量，因为它对光照条件的鲁棒性较弱。在关键帧级别，提取了视觉特征以表征空间信息。在当前算法中，开始和结束帧的颜色直方图用作镜头的视觉特征：</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127211905365.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127211905365"></p>
<p>到这里，一个镜头就可以被表示成：</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127211955881.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127211955881"></p>
<p>即：开始帧，结束帧，活动值（时间特征），开始直方图，结束直方图</p>
<h3 id="时间自适应分组"><a href="#时间自适应分组" class="headerlink" title="时间自适应分组"></a>时间自适应分组</h3><p>在构造场景结构之前，先创建一个中间实体组是很方便的，以利于后续过程。分组的目的是将相似的镜头归为一组，因为相似的镜头很有可能进入同一场景。若要使镜头相似，应满足以下属性。</p>
<p>–视觉相似度相似的镜头应在视觉上相似。也就是说，它们应该具有相似的空间（Hist（bi）和Hist（ei））和时间（Acti）特征。</p>
<p>–时间局部性相似的镜头在时间上应该彼此接近。例如，视觉上相似的镜头，如果时间上彼此相距甚远，则很少属于同一场景，因此<strong>不属于同一组</strong>。</p>
<p>Yeung等人提出了一种用于时间限制的聚类方法来对镜头进行分组，其中如果两个镜头的时间差<strong>大于预定义的阈值</strong>，则将两个镜头之间的相似度设置为0。我们基于上述相似镜头的两个属性<strong>（视觉相似、时间相近）</strong>，提出了一种更通用的<strong>时间自适应分组</strong>方法。在我们提出的方法中，<strong>两个镜头的相似性是视觉相似性的增加函数和帧差异的减小函数。</strong>规定 i 和 j 是要确定相似性的两个镜头的索引，其中镜头j&gt;镜头i。镜头相似度的计算描述如下。</p>
<h4 id="色彩相似度"><a href="#色彩相似度" class="headerlink" title="色彩相似度"></a>色彩相似度</h4><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127212525042.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127212525042">

<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127213528812.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127213528812"></p>
<ol>
<li>色彩相似度（空间特征）：对任意两个镜头的开始结尾做4个计算，相似度是1-$Diff_{x,y}$,其中x&gt;y。</li>
<li>时间引力：baseLength是所有镜头的平均长度乘以一个常数MULTIPLE，用来控制时间吸引力下降的速度。实验中常数=10最好。这里就能知道两个帧之间越近，attr越大；离得越远就越小；如果比平均镜头长度还长，那么直接就是负的了。取了max之后就是0.</li>
<li>将原始相似性转换为适应时间的相似性，从而捕获视觉相似性和时间局部性。其实就是混合，四个组合的时间吸引力和色彩相似度相乘，然后取最大值作为两帧之间的时间自适应颜色度。</li>
</ol>
<h4 id="运动相似度"><a href="#运动相似度" class="headerlink" title="运动相似度"></a>运动相似度</h4><p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127213835805.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127213835805"></p>
<p>$Attr_{center}$是I,J两个镜头中间帧之间的时间引力。乘以两个镜头的活动值的差。</p>
<h4 id="加权和"><a href="#加权和" class="headerlink" title="加权和"></a>加权和</h4><p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127215025162.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127215025162"></p>
<h3 id="场景结构构建"><a href="#场景结构构建" class="headerlink" title="场景结构构建"></a>场景结构构建</h3><p>相似的镜头会被分组，但是如果非相似的镜头之间存在密切的关联，即使是非相似的镜头也可以被分组为一个场景。视频是一种顺序媒体。因此，即使两个或多个过程同时在视频中进行，它们也必须顺序显示，一个接一个地显示。这在电影中很常见。例如，当两个人互相交谈时，即使两个人都参与了对话，电影也会在这两个人之间来回切换。在此示例中，显然存在两个组，一个组对应于人A，另一组对应于人B。尽管这两个组是非相似组，但它们在语义上相关并且构成一个场景。</p>
<p>我们建议使用一种智能的无监督聚类技术来执行场景结构构建。这可以通过两步过程来实现：</p>
<p>–使用时间自适应分组将相似的镜头收集到组中</p>
<p>–将语义相关的组合并到一个统一的场景中。</p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127215805567.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127215805567"></p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127220108842.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127220108842"></p>
<h5 id="cnm，老子看不懂了"><a href="#cnm，老子看不懂了" class="headerlink" title="cnm，老子看不懂了"></a>cnm，老子看不懂了</h5><p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127220720115.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127220720115"></p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127220340917.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127220340917"></p>
<h1 id="视频时序结构图构造"><a href="#视频时序结构图构造" class="headerlink" title="视频时序结构图构造"></a>视频时序结构图构造</h1><p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127221500810.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127221500810"></p>
<p><img src="https://raw.githubusercontent.com/silmont2000/BlogPic/master/img/image-20210127221958438.png" srcset="/silmont2000.github.io/img/loading.gif" alt="image-20210127221958438"></p>

            </div>
            <hr>
            <div>
              <p>
                
                  <span>
                <i class="iconfont icon-inbox"></i>
                    
                      <a class="hover-with-bg" href="/silmont2000.github.io/categories/%E6%96%B0%E7%9F%A5%E6%95%B4%E7%90%86/">新知整理</a>
                      &nbsp;
                    
                  </span>&nbsp;&nbsp;
                
                
                  <span>
                <i class="iconfont icon-tag"></i>
                    
                      <a class="hover-with-bg" href="/silmont2000.github.io/tags/video/">video</a>
                    
                  </span>
                
              </p>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
              
            </div>

            
              <!-- Comments -->
              <div class="comments" id="comments">
                
                
  <div id="lv-container" data-id="city" data-uid="MTAyMC80OTU5NS8yNjA4Ng==">
    <script type="text/javascript">
      (function (d, s) {
        var j, e = d.getElementsByTagName(s)[0];

        if (typeof LivereTower === 'function') {
          return;
        }

        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.defer = true;

        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
    <noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
  </div>


              </div>
            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc-start"></div>
<div id="toc">
  <p class="h5"><i class="far fa-list-alt"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    </div>
    
  <div>
    
      <!-- 不蒜子统计PV -->
      
      <span id="busuanzi_container_site_pv" style="display: none">
      零食总消耗<span id="busuanzi_value_site_pv"></span>袋 :P  
    </span>
    
    
      <!-- 不蒜子统计UV -->
      
      <span id="busuanzi_container_site_uv" style="display: none">
      零食屋总客流量<span id="busuanzi_value_site_uv"></span>小只
    </span>
    
  </div>


    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/popper.js/1.16.1/umd/popper.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="https://cdn.staticfile.org/mdbootstrap/4.13.0/js/mdb.min.js" ></script>
<script  src="/silmont2000.github.io/js/main.js" ></script>


  <script  src="/silmont2000.github.io/js/lazyload.js" ></script>



  
  <script  src="https://cdn.staticfile.org/tocbot/4.10.0/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var navHeight = $('#navbar').height();
      var post = $('#post');
      var toc = $('#toc');
      var tocLimMax = post.offset().top + post.height() - navHeight;

      $(window).scroll(function () {
        var tocLimMin = $('#toc-start').offset().top - navHeight;
        var scroH = document.body.scrollTop + document.documentElement.scrollTop;

        if (tocLimMin <= scroH && scroH <= tocLimMax) {
          toc.css({
            'display': 'block',
            'position': 'fixed',
            'top': navHeight,
          });
        } else if (scroH <= tocLimMin) {
          toc.css({
            'position': '',
            'top': '',
          });
        } else if (scroH > tocLimMax) {
          toc.css('display', 'none');
        }
      });
      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '.post-content',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        scrollSmooth: true,
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc > p').css('visibility', 'visible');
      }
      var offset = $('#board-ctn').css('margin-right')
      $('#toc-ctn').css({
        'right': offset
      })
    });
  </script>







  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




<!-- Plugins -->


  

  

  

  

  



  <script  src="https://cdn.staticfile.org/prettify/188.0.0/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "数字视音频-视频&nbsp;",
      ],
      cursorChar: "：P",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/silmont2000.github.io/js/local-search.js" ></script>
  <script>
    var path = "/silmont2000.github.io/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script defer src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>













</body>
</html>
